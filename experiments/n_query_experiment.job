#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=n_query_experiment_100_epochs_100_rounds_10_n_query_realistic
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=48:00:00
#SBATCH --output=../out/slurm_output_%x.out

module purge
module load 2022
module load Anaconda3/2022.05

# Activate conda environment
source activate active-learning-luca

# Change to working directory
cd $HOME/active-learning-luca

# Ensure wandb online
wandb online

# MNIST:  60,000 training images and 10,000 testing images
n_init_labeled_mnist=(10000 20000 30000 40000 50000)

# MNIST:  60,000 training images and 10,000 testing images
n_init_labeled_mnist_realistic=(10 30 50 100)

# CIFAR-10: 50,000 training images and 10,000 testing images
n_init_labeled_cifar=(10000 20000 30000 40000)

# Tiny-ImageNet: 100,000 training images and 10,000 testing images
n_init_labeled_tiny_imagenet=(10000 20000 30000 40000 50000 60000 70000 80000 90000)

n_init_labeled=$n_init_labeled_mnist_realistic

for n in "${n_init_labeled[@]}"
    do
        python main.py --experiment --strategy random --epochs 100 --n_init_labeled $n --dataset mnist --model lenet --n_query 10 --n_round 100 
        # python main.py --epochs 100 --n_init_labeled $n --dataset mnist --model resnet18 --n_query 1000 
        # python main.py --epochs 100 --n_init_labeled $n --dataset mnist --model resnet34 --n_query 1000 
    done